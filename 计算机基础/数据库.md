# 数据库（MySQL）

## mysql基础

主键，外键与索引：

​	主键——唯⼀标识⼀条记录，不允许重复，不允许为空。用来保存数据的完整性

​	外键——外键表⽰另⼀张表的主键，允许重复，可以是空值。主要⽤于和其它表建⽴联系。

​	索引——没有重复值，但可以有⼀个空值。为了提⾼查询排序的速度

表的优化策略：

​	1.尽量使⽤TINYINT，SMALLINT，MEDIUM_INT替代INT类型，如果是⾮负则加上UNSIGNED 

​	2.VARCHAR的长度只分配真正需要的空间

​	3.尽量使⽤整数或者枚举替代字符串类型

​	4.时间类型尽量使⽤TIMESTAMP⽽⾮DATETIME

​	5.单表不要放太多字段

​	6.尽量少使⽤NULL，很难查询优化⽽且占⽤额外索引空间

MySQL和MongoDB的区别：

​	1.从数据库模型来说，前者为关系型，后者为非关系型

​	2.存储方式上，前者针对不同引擎的存储方式不同，后者主要使用虚拟内存+持久化

​	3.查询语句上，MySQL有专属的sql语句，MongoDB也有自己的查询方式

​	4.常见架构上，MySQL包含单点，Master-Slave，MHA，Cluster；MongoDB通过副本集与分⽚来实现⾼可⽤

```
单点：数据库系统中的一个独立的、不可替代的节点，所有的请求都集中在这个节点上。这种架构存在单点故障的风险，因为如果单点发生故障，整个系统可能会受到影响。

Master-Slave（主从）：主从架构包括一个主数据库（Master）和多个从数据库（Slave）。主数据库负责处理写操作和同步数据到从数据库，而从数据库主要用于读取，提高读取性能和冗余备份。这种架构减轻了单点故障的压力，但主数据库仍然是关键节点。

MHA（MySQL Master High Availability，MySQL高可用）：通过监视主数据库的状态并在发生故障时自动切换到备用主数据库，从而减少停机时间。MHA的目标是实现主数据库的快速故障转移和自动恢复。本质上和Redis中的哨兵类似。

Cluster（集群）：集群是将多个计算机或服务器连接在一起，形成一个单一的系统，共同工作以提供更高的性能和可用性。在MySQL中，可以使用集群来水平扩展数据库性能，将数据分布在多个节点上。MySQL Cluster是一个用于实现高可用性和分布式数据库的解决方案。
```

​	5.数据处理方式上，MySQL的每个引擎有不同的处理特点，MongoDB则基于内存，将热数据存储在物理内存中，从⽽达到⾼速读写的⽬的

​	6.在数据存储上，MySQL效率相对较低，但是MongoDB不⽀持事务 



## mysql表连接

内/外连接：

​	内连接：驱动表中的记录在被驱动表中找不到记录，那么驱动表的这条记录也不会加入到最后的结果（两表的位置可以交换）

```sql
select * from 驱动表 join 被驱动表;
select * from 驱动表 inner join 被驱动表;
select * from 驱动表 cross join 被驱动表;
```

​	外连接：驱动表的记录在被驱动表中找不到，也要加入到最后的结果（两表的位置一定不能换，因为驱动表的内容需要保留）

```sql
select * from 驱动表 left join 被驱动表 on 连接条件;
select * from 被驱动表 right join 驱动表 on 连接条件;
```

​	使用的连接的时候常常需要过滤条件，分为on和where两种：

​	on：在内连接中与where等价；在外连接中，如果驱动表中的记录在被驱动表中没有记录可以匹配，该驱动表记录仍会加⼊到结果中，对应的被驱动表字段以null填充

​	where：不论内外连接，只要是不符合where⼦句的记录都不会加⼊到最后的结果中



## mysql锁

### 锁的种类

1.全局锁，即对整个数据库实例加锁。

​	典型使⽤场景：全库逻辑备份，即把整个库的表都select出来存成⽂本。

​	MySQL 提供了⼀个加全局读锁的⽅法，命令是 Flush tables with read lock (FTWRL)。 

​	当你需要让整个库处于只读状态的时候，可以使⽤这个命令，之后其他线程的以下语句会被阻塞： 

​	1. 数据更新语句（数据的增删改） 2. 数据定义语句（包括建表、修改表结构等） 3. 更新类事务的提交语句

2.表级锁，包括两种，分别是表锁和元数据锁（meta data lock，MDL）

​	表锁：每次操作锁住整张表，开销少，加锁快，同时并发程度最低

​	表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以⽤ unlock tables 主动释放锁，也可以在客户端断开的时候⾃动释放。

​	需要注意： lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。

​	元数据锁：MDL 不需要显式使⽤，在访问⼀个表的时候会被⾃动加上。MDL 的作⽤：保证读写的正确性。

​	当对⼀个表做增删改查操作的时候，加 MDL 读锁； 当要对表做结构变更操作的时候，加 MDL 写锁。

​	读锁之间不互斥，因此你可以有多个线程同时对⼀张表增删改查。读写锁之间、写锁之间是互斥的，⽤来保证变更表结构操作的安全性。因此，如果有两个线程要同时给⼀个表加字段，其中⼀个要等另⼀个执⾏完才能开始执⾏。

​	事务中的MDL 锁，在语句执⾏开始时申请，但是语句结束后并不会马上释放，⽽会等到整个事务提交后再释放。 （这可能会产⽣死锁的问题）

3.行锁，针对数据表中⾏记录的锁（也有⼈称为记录锁）

​	每次操作锁住一行数据；开销大，加锁慢；但是发生锁冲突的概率较低，并发度也是最高的

​	在 InnoDB 事务中，⾏锁是在需要的时候才加上的，但并不是不需要了就⽴刻释放，⽽是要等到事务结束时才释放。这个就是两阶段锁协议。

​	如果事务中需要锁多个⾏，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放，以提高并发性。

### 锁的划分

#### 数据库角度

1.共享锁（也叫S锁或读锁）

​	共享锁锁定的资源可以被其他⽤户读取，但不能修改。

​	在进⾏SELECT的时候，会将对象进⾏共享锁锁定，当数据读取完毕之后，就会释放共享锁，这样就可以保证数据 在读取时不被修改。

​	如果我们想要给某⼀⾏加上共享锁呢，⽐如想对 user_id=10的数据⾏加上共享锁，可以像下⾯这样：

```sql
SELECT user_id FROM product_comment WHERE user_id = 10 LOCK IN SHARE MODE;
```

2.排他锁（也叫X锁，独占锁，写锁）

​	排它锁锁定的数据只允许进⾏锁定操作的事务使⽤，其他事务⽆法对已锁定的数据进⾏查询或修改。

​	如果我们想要在某个数据⾏上添加排它锁，⽐如针对 user_id=10的数据⾏，则写成如下这样：

```sql
SELECT user_id FROM product_comment WHERE user_id = 10 FOR UPDATE;
```

另外当我们对数据进⾏更新的时候，也就是INSERT、DELETE或者UPDATE的时候，数据库也会⾃动使⽤排它锁， 防⽌其他事务对该数据⾏进⾏操作。

3.表级使用共享锁或排他锁

​	共享锁对表的加锁和解锁

```sql
LOCK TABLE product_comment READ; 
UNLOCK TABLE; 
```

​	排他锁对表的加锁和解锁

```sql
LOCK TABLE product_comment WRITE; 
UNLOCK TABLE;
```

4.意向锁

如果我们给某⼀⾏数据加上了排它锁，数据库会⾃动给更⼤⼀级的空间，⽐如数据页或数据表 加上意向锁，告诉其他⼈这个数据页或数据表已经有⼈上过排它锁了，这样当其他⼈想要获取数据表排它锁的时候，只需要了解是否有⼈已经获取了这个数据表的意向排他锁即可。

#### 程序员角度

1.乐观锁

​	认为对同⼀数据的并发操作不会总发⽣，属于⼩概率事件，不⽤每次都对数据上锁，也就是不采⽤数据库⾃⾝的锁 机制，⽽是通过程序来实现。在程序上，我们可以采⽤版本号机制或者时间戳机制实现。

​	适合读操作多的场景，相对来说写的操作⽐较少。它的优点在于程序实现，不存在死锁问题，不过适⽤场景也会相对乐观，因为它阻⽌不了除了程序以外的数据库操作。

2.悲观锁

​	对数据被其他事务的修改持保守态度，会通过数据库⾃⾝的锁机制来实现，从⽽保证数据操作的排它性。

​	适合写操作多的场景，因为写的操作具有排它性。采⽤悲观锁的⽅式，可以在数据库层⾯阻⽌其他事务对该数据的 操作权限，防⽌读 - 写和写 - 写的冲突。但是加锁的时间会⽐较长，可能会长时间限制其他⽤户的访问，也就是说他的并发访问性不好。



## mysql事务

事务是⼀个最⼩的不可再分的⼯作单元；通常情况下，⼀个事务对应⼀个完整的业务。

mysql事务特性：ACID

​	A：原⼦性（Atomicity），⼀个事务的所有操作，要么全部完成，要么都没完成，不能结束在中间环节。如果事务在执⾏过程中发⽣错误，会被回滚到事务开始之前的状态

​	C：⼀致性（Consistency），在事务开始之前以及事务结束之后，数据库的完整性不能被破坏 

​	I：隔离性（Isolation），允许多个并发事务同时对数据进⾏修改和读写的能⼒，它可以防⽌由于多个事务并发执⾏时由于交叉执⾏⽽导致的数据不⼀致 

​	D：持久性（Durability），事务处理结束了以后，对数据的修改是永久的，即使是发⽣了系统故障，数据也不会丢失

MVCC（多版本并发控制）四大问题：脏写，脏读，不可重复读，幻读

MVCC四大隔离级别：

​	RU读不提交：事务之间完全不隔离，会产⽣脏读，⼀般情况不会使⽤ 

​	RC 读提交：本事务读取到的是其它事务提交的最新数据，但有⼀个问题在同⼀个事务中，前后两相同的select可能会读到不同的结果

​	RR可重复度：在同⼀个事务中，select的结果是事务开始时时间点的状态，因此，同⼀个事务同样的select操作可以读到⼀致的结 果

​	serializable串行化：隐式获取共享锁，保证不同事务之间的互斥



## mysql索引

索引的出现其实就是为了提⾼数据查询的效率，就像书的⽬录⼀样

MySQL 在查询⽅⾯主要就是两种⽅式：1.全表扫描（⼀个⼀个挨个找） 2.根据索引检索

```sql
create index 索引名 on 表名(列名);
# 也可以有多个列名构成索引（即，组合索引）
drop index 索引名 on 表名;
```

使⽤索引可以⼤⼤加快数据的检索速度（⼤⼤减少检索的数据量），这也是创建索引的最主要的原因。

 但是注意使⽤索引不⼀定能够提⾼查询性能，因为如果数据库的数据量不⼤，那么使⽤索引也不⼀定能够带来很⼤提升。其余⼤多数情况下，索引查询⽐全表扫描要快。 通过创建唯⼀性索引，可以保证数据库表中每⼀⾏数据的唯⼀性。



## mysql查询优化

1.条件化简

2.移除不必要的括号

3.常量传递

4.移除没用的条件：例如一定为true或false

5.表达式计算：

​	（1）表达式只包含常量的话，值会被计算出来

​	（2）如果某个列在函数中或者以运算形式出现，优化器不会进⾏化简

6.having和where⼦句的合并：

​	（1）查询语句中没有sum、max这样的聚集函数以及group⼦句的话，优化器会将having和where⼦句合并

​	（2）having⼦句⽤于分组后过滤，where⽤于分组前合并

7.常量表检测：

​	（1）使⽤主键等值匹配、使⽤唯⼀⼆级索引列等值匹配进⾏查询的表称为常量表

​	（2）优化器会优先执⾏常量表查询，因为速度⾮常快

8.外连接消除

9.优化器会将右连接转化为左连接

10.空值拒绝：

​	（1）在外连接查询中，指定的where⼦句中包含被驱动表的列不为null值的条件（就是不允许查出来的记录中含有null值）

​	（2）被驱动表的where⼦句符合空值拒绝的条件后，外连接和内连接可以相互转换

11.子查询优化



## mysql子查询

在⼀个查询语句中的某个位置可以出现另⼀个查询语句，这另⼀个查询就叫⼦查询

（1）按出现位置分类

```sql
#在select子句中
select (select m1 from t1 limit 1);
#在from子句中，将这种⼦查询的结果当做⼀个表，在from⼦句中的⼦查询称为派⽣表
select m, n from (select ...);
#在where或on子句中
select * from t1 where m1 in (select ...);
```

（2）按返回的结果集分类

```sql
#标量子查询，只返回一个单一值
select (select m1 from t1 limit 1);
#行子查询，返回一条记录，需要包含多个列，使⽤limit 1保证⼦查询的结果只有⼀条记录
select * from t1 where (m1, n1) = (select m2, n2 from t2 limit 1);
#列子查询，返回⼀个列的数据，可能包含多条纪录
select * from t1 where m1 in (select m2 from t2);
#表子查询，⼦查询的结果既有多条纪录，又有多个列
select * from t1 where (m1, n1) = (select m2, n2 from t2);
```

（3）按外层查询关系分类

```sql
#不相关⼦查询,⼦查询可以单独运⾏出结果，不依赖于外层查询的值

#相关子查询，子查询的执行需要依赖外部查询的值
select * from t1 where m1 in (select m2 from t2 where n1 = n2);
```



## mysql高性能

### mysql主从复制

数据可以从⼀个MySQL数据库服务器主节点复制到⼀个或者多个从节点。 

MySQL默认采⽤异步复制⽅式，这样从节点就不⽤⼀直访问主服务器来更新最新数据。 

从节点可以复制主节点数据库中的所有数据库、特定的数据库或者特定的表。

用途：

​	1.数据实时备份

​	当系统中某个节点发⽣故障时，可以⽅便故障切换

​	2.读写分离

​	在开发过程中，如果遇到某个sql语句需要锁表，导致暂时不能使⽤读的服务

​	使⽤主从复制，让主数据库负责写，从数据库负责读，即使主库出现锁表的情景，也可以通过从库正常读数据

​	3.架构扩展

​	随着系统中业务访问量的增加，如果是单机部署数据，会导致I/O访问频率过⾼

​	通过主从复制，增加多个数据存储结点，将负载分布在多个从节点上，降低单机的I/O访问频率，提⾼单机的I/O性能

原理：

mysql主从复制涉及到三个线程，一个运行在主节点（binary log dump thread），两个运行在从节点（I/O thread、SQL thread）

binary log dump thread：

​	当从节点连接主节点的时候，主节点创建该线程，⽤于发送bin-log内容

I/O thread：

​	当从节点执⾏“start slave”命令之后，从节点会创建⼀个I/O线程⽤来连接主节点，请求其中的数据。I/O线程接收到主节点binlog dump的更新数据之后，保存在本地的relay log中 

SQL thread：

​	该线程负责读取relay log中的内容，解析或具体的操作并执⾏，最终保证主从数据的⼀致性

基本过程：

1.从节点I/O进程连接主节点

请求指定⽇志⽂件的指定位置后⾯的内容 

2.主节点接收到请求之后

通过负责复制的I/O进程根据请求的信息读取指定的⽇志位置之后的⽇志信息，返回给从节点。返回信息中除了⽇ 志所包含的指定⽇志信息还包含了本次返回信息的 bin-log file 以及 bin-log position

3.从节点的I/O线程接收到内容之后

将接收到的⽇志内容更新到本机的 relay log 中，并且把读取到的 binary log ⽂件名和位置保存到 master-info ⽂件 中，⽅便下⼀次告知 master 从节点需要更新的位置

4.Slave 的 SQL 线程检测到 relay-log 中新增了内容

将 relay-log 的内容解析成在主节点上实际执⾏的操作，并在数据库中执⾏

主从复制模式：

1.异步模式

​	主服务器将更新操作写入自己的日志文件，而从服务器则异步地从主服务器的日志文件读取并应用这些更新。

​	异步模式下，主服务器不等待从服务器的响应，而是继续处理其他请求。这意味着主从服务器之间的数据可能存在一定程度的延迟。

2.半同步模式

​	主服务器在将更新写入自己的日志文件后，等待至少一个从服务器接收并确认接收了这个更新，然后主服务器才继续。

​	半同步模式相对于异步模式来说，提高了数据一致性，因为主服务器会等待至少一个从服务器确认接收更新。

3.全同步模式

​	主服务器在将更新写入自己的日志文件后，等待所有的从服务器都接收并确认接收了这个更新，然后主服务器才继续。

​	全同步模式要求所有的从服务器都接收到更新，因此相对于半同步模式来说，提高了更高的数据一致性，但也增加了延迟。

4.GTID复制模式

​	GTID是MySQL 5.6及以上版本引入的一项特性，用于跟踪和标识复制中的事务。GTID复制模式使用全局唯一的标识符来标记每个事务，使得复制更加可靠和容错。

​	GTID模式简化了复制配置，更容易管理和维护。它还提供了在复制中进行故障切换时更容易恢复的好处。

### 分库分表

原因：

​	1.单库太大，单个数据库处理能⼒有限，所在的服务器上的磁盘空间也有限，单库存在I/O操作瓶颈。 因此需要切分成更多更⼩的库

​	2.单表太大，索引膨胀，查询超时。因此需要切分成多个数据集更小的表

拆分方案：

1.垂直拆分

​	（1）垂直分表

​	“⼤表拆⼩表“，基于列的字段进⾏

​	⼀般表中字段较多，将不常⽤的，数据较⼤的，长度较长的，拆分到“扩展表” 

​	（2）垂直分库

​	针对⼀个系统中不同业务进⾏拆分，拆分之后放到多个服务器上  

2.水平拆分

​	（1）水平分表

​	针对数据量巨⼤的单张表（⽐如订单），按照某种规则（RANGE，HASH取模），切分到多张表中，这些表还在 ⼀个数据库中

​	（2）水平分库

​	将单张表的数据切分到多个服务器上，每个服务器都有相应的库和表，只是表中的数据集合不同

​	⽔平分库能够有效的缓解单机和单库的性能瓶颈，I/O，连接数和硬件资源等瓶颈

​	⽔平分库分表切分规则：

​		1.range： 根据范围，⽐如0-1000⼀个表，1001到2000⼀个表

​		2.hash取模： ⽐如取ID，进⾏hash取模，根据模数分配到不同的数据库中

​		3.地理区域/时间范围： 按照地理范围/时间进⾏划分



## MySQL常见问题

### 幻读与锁

1.什么是幻读

幻读是指在同⼀个事务中，存在前后两次查询同⼀个范围的数据，但是第⼆次查询却看到了第⼀次查询没看到的⾏

出现的场景是：a.事务的隔离级别为可重复读，且是当前读。  b.幻读仅专指新插⼊的⾏

2.幻读带来的问题

对⾏锁语义的破坏，破坏了数据的一致性

3.如何避免幻读

存储引擎采⽤加间隙锁的⽅式来避免出现幻读

4.为啥会出现幻读

⾏锁只能锁定存在的⾏，针对新插⼊的操作没有限定

5.什么是间隙锁？间隙锁如何避免幻读？它引入了什么新的问题

间隙锁是专门⽤于解决幻读这种问题的锁，它锁的了⾏与⾏之间的间隙，能够阻塞新插⼊的操作

间隙锁的引⼊也带来了⼀些新的问题，⽐如：降低并发度，可能导致死锁

间隙锁之间是不冲突的，间隙锁会阻塞插⼊操作。另外，间隙锁在可重复读级别下才是有效的

⾏锁和间隙锁合称 next-key lock，这个锁是左开右闭的区间。

MySQL 为了解决幻读问题，在线程更新数据并 next-key lock 的过程中，⾸先必须在可重复读的隔离级别下，执⾏以下的原则和优化：

​	原则：1.加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间

​		  2.查找过程中访问到的对象才会加锁

​	优化：1.索引上的等值查询，给唯⼀索引加锁的时候，next-key lock 退化为⾏锁，如果不存在这个索引，退化为间隙锁

​		   2.索引上的等值查询，向右遍历时且最后⼀个值不满⾜等值条件的时候，next-key lock 退化为间隙锁

⾮唯⼀索引的范围查询：范围查询都会访问到不满⾜条件的第⼀个值，并且不会执⾏上述的两个优化。唯⼀索引的范围查询仍旧会执⾏上述的优化

delete 语句的和查询的加锁⽅式相同

limit 语句，遍历到满⾜条件的 n 条数据后，之后不再加 next-key lock

如果两个都给同⼀个间隙上锁，之后两个线程都没办法在这个间隙上更新数据了，都会陷⼊等待另⼀个线程的间隙锁释放，也就是死锁

如果使⽤读提交隔离级别，那么只加⾏锁，不加间隙锁，语句执⾏过程中加上的⾏锁，在语句执⾏完成后，就要把 “不满⾜条件的⾏”上的⾏锁直接释放了，不需要等到事务提交才释放



### 事务相关

1.for update的使用场景

for update会进行数据加锁，防止高并发的情况下数据出错，即使事务保持当前读的状态。

for update仅适用于InnoDB，且必须在事务块(BEGIN/COMMIT)中才能生效。在进行事务操作时，通过“for update”语句，MySQL会对查询结果集中每行数据都添加排他锁，其他线程对该记录的更新与删除操作都会阻塞。排他锁包含行锁、表锁。

2.事务隔离

innodb支持RC和RR隔离级别的实现用的是一致性视图

事务在启动时会拍⼀个快照,这个快照是基于整个库的。基于整个库的意思就是说⼀个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)

如果在事务内 select 表,另外的事务执⾏了DDL（数据定义语言操作，包括表的创建和删除，修改表结构等）表,根据发⽣时间,只有两种情况：报错或者锁住

3.事务如何实行多版本并发控制（MVCC）

每个事务都有一个事务ID，叫做transaction id(严格递增) 

事务在启动时,找到已提交的最⼤事务ID记为up_limit_id

事务在更新⼀条语句时，首先，（更新前）会将原始数据写到 undo log （记录事务修改之前数据的机制，以便在回滚或撤销操作时使用）⾥；然后再对数据页进行相应更改；更新后，数据库会在被修改的记录的行头（record header）中记录执行该修改的事务的ID。这是为了跟踪记录的修改历史，以便实现并发控制和事务隔离。

一个事务要查看⼀条数据时,必须先⽤该事务的 up_limit_id（事务的上界ID） 与该⾏的transaction id 做⽐对：

​	如果 up_limit_id >= transaction id,那么可以看

​	如果 up_limit_id < transaction id,则只能去 undo log ⾥去取

去 undo log 查找数据的时候,也需要做⽐对,必须 up_limit_id > transaction id，才返回数据

如果 up_limit_id <= transaction id，则可能表示该数据已经是事务能够查看的最新版本，无法从undo log中进行恢复

4.什么是当前读

由于当前读都是先读后写,只能读当前的值,所以为当前读会更新事务内的 up_limit_id 为该事务的 transaction

5.为什么 rr 能实现可重复读⽽ rc 不能

分两种情况：

​	1.快照读的情况下，rr不能更新事务内的 up_limit_id ,⽽ rc 每次会把 up_limit_id 更新为快照读之前最新已提交事务的 transaction id,则 rc 不能可重复读 

​	2.当前读的情况下，rr 是利⽤ record lock+gap lock来实现的,⽽ rc 没有 gap,所以 rc 不能可重复读

6.普通索引和唯一索引的区别

对于查询过程来说：

​	普通索引查到满⾜条件的第⼀个记录后，继续查找下⼀个记录，直到第⼀个不满⾜条件的记录

​	由于索引唯⼀性，查到第⼀个满⾜条件的记录后，停⽌检索  但是，两者的性能差距微乎其微。因为InnoDB根据数据页来读写的。

对于更新过程来说：

​	唯一索引不能使用change buffer，而它对性能提升非常明显，因此更优先使用普通索引

change buffer：

​	当需要更新一个数据页时，如果数据页在内存中就直接更新；如果不在内存中，在不影响数据⼀致性的前提下，InnoDB 会将这些更新操作缓存在change buffer 中

​	下次查询需要访问这个数据页的时候，将数据页读⼊内存，然后执⾏ change buffer 中的与这个页有关的操作

​	change buffer 是可以持久化的数据。在内存中有拷贝，也会被写⼊到磁盘上

​	change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显

​	将 change buffer 中的操作应⽤到原数据页上，得到最新结果的过程，称为 purge。访问这个数据页会触发 purge，系统有后台线程定期 purge，在数据库正常关闭的过程中，也会执⾏ purge。

​	在⼀个数据页做purge之前，change buffer记录的变更越多，收益就越⼤。对于写多读少的业务来说，页⾯在写完以后马上被访问到的概率⽐较⼩，此时change buffer的使⽤效果最好。这种 业务模型常见的就是账单类、⽇志类的系统。相反，对于写少读多的业务来说，随机访问IO的次数不会减少，反⽽增加了change buffer的维护代价。

​	redo log（记录已经执行的修改操作，undo log记录的是事务回滚前的数据版本）主要节省的是随机写磁盘的IO消耗(将事务的修改操作以顺序写的方式记录到 Redo Log 中，最小化对磁盘的随机写入)，⽽change buffer主要节省的则是随机读磁盘的IO消耗。



### 数据库记录相关

1.char和varchar的区别

char是固定长度类型，即便存入的字符大小比声明的大小小，实际占据的大小仍然为声明的大小。

varchar则可变，变长字段所占用的字节数被单独放在变长字段列表中，并且按照表的顺序逆序存放，因此仍然需要额外的存储。

2.mysql抖一下是什么情况

偶尔慢一下，此时mysql的运行不稳定，需要花费更多的资需源处理别的事情，会使SQL语句的执⾏效率明显变慢

InnoDB内mysql抖的原因是InnoDB 会在后台刷脏页，⽽刷脏页的过程是要将内存页写⼊磁盘。在这种情况下，刷脏页的逻辑会占⽤ IO 资源并可能影响到了更新语句，导致突然慢一下

mysql突然变慢导致的性能下降对业务是很不友好的

让mysql不抖的核心在两点：

​	1.设置合理参数配配置，尤其是设置好innodb_io_capacity 的值（代表系统的期望 I/O 能力。这个值是一个正整数，单位是 I/O 操作每秒（IOPS）。通过设置这个参数，你可以告诉 InnoDB 引擎系统的磁盘性能水平，以便在执行各种操作时更好地调整其行为）

​	2.多关注脏页⽐例，不要让它经常接近 75%（优化查询和事务，调整事务大小，适当调整缓冲池大小，使用合理的刷新策略，增加磁盘性能）

3.脏页/干净页是什么

脏页：当内存数据页跟磁盘数据页内容不⼀致的时候，我们称这个内存页为“脏页”

干净页：内存数据写⼊到磁盘后，内存和磁盘上的数据页的内容就⼀致了，称为“⼲净页”

4.为什么会产生脏页

因为使⽤了WAL技术，这个技术会把数据库的随机写转化为顺序写，但副作⽤就是会产⽣脏页。

Write-Ahead Logging 是一种事务处理的机制，它要求在修改数据库内容之前，先将修改的操作记录到一个称为日志（WAL日志）的地方。

WAL 日志是一个顺序写的日志文件，其中记录了事务对数据库的修改操作。在事务提交前，WAL 日志中的记录已经写入磁盘，表示该事务的修改操作已经持久化。

当使用 WAL 技术时，事务的修改操作首先被写入 WAL 日志，这个过程是顺序写入的，因为 WAL 是一个追加写入的顺序文件。这意味着，即使实际数据库的数据页是通过随机写进行修改，WAL 日志的写入是按照顺序进行的。这有助于减少磁盘寻址的开销，提高写入性能。

5.随机写/顺序写

随机写指的是数据在存储介质上的写入位置不是连续的，而是分散在不同的位置。每次写入都可能导致磁头的移动到不同的磁道或扇区，因此需要进行随机寻址。在随机写的情况下，磁盘的读写头需要频繁地在不同的位置进行跳跃，导致更高的磁盘访问延迟。

顺序写指的是数据在存储介质上的写入位置是连续的，按照顺序进行。数据依次写入相邻的位置，使得磁头的移动是顺序的，无需频繁的寻址。顺序写通常具有更高的性能，因为它可以充分利用磁盘的带宽。顺序写减少了磁头的寻址成本，减小了磁盘读写的延迟。

写redo log是顺序写的，先写redo log等合适的时候再写磁盘，间接的将随机写变成了顺序写，性能确实会提⾼不 少。



### 数据库信息相关

1.当删除表的数据后，表文件的大小为什么不会有变化

因为delete 命令其实只是把记录的位置，或者数据页标记为了“可复⽤”，但磁盘⽂件的⼤⼩是不会变的。也可以认为是⼀种逻辑删除，所以物理空间没有实际释放，只是标记为可复⽤，表⽂件的⼤⼩不变

2.表的数据信息/结构信息存在哪里

数据信息：存储在共享表空间⾥，也可以单独存储在⼀个以.ibd为后缀的⽂件⾥。在使用文件管理时，使⽤ drop table 命令也能直接把对应的⽂件删除，如果存储在共享空间之中即使表删除了空间也不会释放

结构信息：存储在系统数据表中，主要⽤于存储MySQL的系统数据，⽐如：数据字典、undo log(默认)等⽂件

3.如何才能真正在表删除删除后，对应的表文件大小减小

```sql
# optimize(优化)等于 recreate+analyze，会重建一个新的，紧凑的表，有助于减少表碎片并提高性能 
optimize table t
# truncate（清空）等于 drop+create，会将原表中数据全部删除，但保留表的结构
truncate table t 
```

4.什么是空洞

空洞就是那些被标记可复⽤但是还没被使⽤的存储空间。

​	使⽤delete命令删除数据会产⽣空洞，标记为可复⽤。

​	插⼊新的数据可能引起页分裂，也可能产⽣空洞  

​	修改操作，有时⼀些先删后插的动作也可能产⽣空洞

5.count()计数相关

count使用的形式：

​	1.count(字段)，表示对某个特定字段（列）进行计数。它将计算该字段非空的行数，不包括字段值为 NULL 的行

​	2.count(主键id)，表示对主键 id 进行计数。主键 id 通常是一个唯一标识符，不应该包含 NULL 值。这个计数将统计表中主键列非空的行数

​	3.count(1)和count(*)，都表示计算所有行的数量，包括包含 NULL 值的行

每个存储引擎对count的实现不同，对于innodb来说，count() 是⼀个聚合函数，对于返回的结果集，⼀⾏⾏地判断，如果 count 函数的参数不是 NULL，累计值就加 1， 否则不加，最后返回累计值



### 数据库语句和存储相关

1.order by是怎样工作的

涉及到⽤户语句的排序，mysql 会给每个线程分配⼀块内存⽤于排序，也就是 sort_buffer。

```sql
select city,name,age from t where city='杭州' order by name limit 1000;
```

这条语句的执⾏逻辑是：

​	1.先初始化 sort_bufer

​	2.然后放⼊ city,name,age 字段，不断地由主键id索引到整⾏再到三个字段的值，匹配查找的值存⼊ sort_buff

​	3.然后按 name 排序，返回前 1000 个值

但是如果 sort_buffer_size 设置的太⼩，⽆法存放所有匹配的字段，排序就⽆法在内存中完成

如果要记录的字段太长，这样内存⾥能够同时放下的⾏数很少，要分成很多个临时⽂件，排序的性能会很差

这时会换⼀个算法，叫做rowid排序，顾名思义，就是对主键 id 以及排序字段进⾏存放，这样就节省了空间，但是 最后需要通过主键 id 去找到之前未取出的字段。对⽐全字段排序，rowid 排序多访问了⼀次表 t 的主键索引。（是否进行rowid排序是根据sort_buffer的大小决定的）

2.SQL语句性能差异巨大的原因

 	1.条件字段的函数操作，对索引字段对函数进程操作，可能会破坏索引值的有序性，因此优化器就决定放弃⾛树搜索功能，转⽽进⾏全表扫描，所以运⾏就变慢了

​	2.隐式类型转换，同样会额外调用函数（例如为了避免错误将字符串隐式转换为整数值），导致优化器放弃B+树搜索，转而使用全表搜索

​	3.隐式字符编码转换，如果执⾏两个表的联合查询，两个表通过外键进⾏联结，如果两个表使⽤的字符集不同，会对低⽔平的字符集执⾏升级转换函数，这时优化器也会转而使用全表搜索

总结：对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃⾛树搜索功能

3.只查询一行语句也很慢的原因

从大到小分为三种情况

​	1.MySQL 数据库本⾝被堵住了，⽐如：系统或⽹络资源不够

​	2.SQL 语句被堵住了，⽐如：表锁，⾏锁等，导致存储引擎不执⾏对应的 SQL 语句

​	3.索引使⽤不当，没有⾛索引

​	4.是表中数据的特点导致的，⾛了索引，但由于⼀致性读，需要回滚多次才能读取出当前事务对应的值

4.临时表为什么可以重名

由于临时表是每个线程⾃⼰可见的，所以不需要考虑多个线程执⾏同⼀个处理逻辑时，临时表的重名问题。在线程 退出的时候，临时表也能⾃动删除，省去了收尾和异常处理的⼯作。

这⾥说到的临时表是⽤户临时表，⽽不是内存临时表

5.group-by使⽤原则

​	1.如果对 group by 语句的结果没有排序要求，要在语句后⾯加 order by null；

​	2.尽量让 group by 过程⽤上表的索引，确认⽅法是 explain 结果⾥没有 Using temporary 和 Using filesort；

​	3.如果 group by 需要统计的数据量不⼤，尽量只使⽤内存临时表；也可以通过适当调⼤ tmp_table_size 参数，来避免⽤到磁盘临时表；

​	4.如果数据量实在太⼤，使⽤ SQL_BIG_RESULT 这个提⽰，来告诉优化器直接使⽤排序算法得到 group by 的结果， 不要使⽤内存临时表。

6.一张表可以存储多少条信息

假设采⽤的是InnoDB引擎，本质上是在求⼀个 B+树 能够存储多少数据，根据 B+树 定义也就转变成了2个⼦问题

​	1.每个叶⼦结点存储能存储多少条记录

​		假设⼀条记录的⼤⼩为1kb，InnoDB中默认每页为16kb，则⼀个叶⼦节点可存储16条数据 

​	2.⼀共有多少个叶⼦结点 

​		根据B+树的形式，确定了该B+树为⼏阶的B+树，以及树的⾼度，就能确定叶⼦节点有最多有多少个

​		⾮叶⼦节点存储的数据为关键字+指针。关键字也就是我们常说的主键，如果为int类型则4字节，指针⼤⼩也假设为4字节，那么⼀个⾮叶⼦节点最多能够存储 16384（默认大小，单位为字节） / 8 = 2048 个，同时，每一个非叶子节点的指针对应一个叶子节点，最终的存储数量为2048 X 16 = 32768条记录（索引高度为两层）

7. 有⼀个联合索引(A,B,C),select * from test where a,c会不会使⽤索引？

不会，根据索引的创建过程，这棵 b+ 树的叶⼦节点的记录以及 ⼆级索引记录 是按照 abc 的规则来排序的，也就是 说，先按照 a的⼤⼩来排序，a 相等的情况下，再按照 b 的⼤⼩来排序，b 相等的话，最后再是 c 。这样的话，符合 where 条件的记录不⼀定相邻，查询效率并没有变高，还是得一条一条查。



# 数据库（Redis）

## Redis基本结构

1.String

可以是字符串、整数或者浮点数。

Redis 的字符串有两种存储⽅式，在长度特别短时，使⽤ embstr 形式存储(embeded)，⽽当长度超过 44 字节时， 使⽤ raw 形式存储。

命令：get、set、del

2.list

一个链表，链表上的每个节点都包含了⼀个字符串 

命令：rpush、lrange、index、lpop

3.set

包含字符串的⽆序收集器，并且被包含的每个字符串都是独⼀⽆⼆

命令：sadd、smember、sismember、srem

4.hash

包含键值对的⽆序散列表

命令：hset，hget，hgetall，hdel

5.zset：

字符串的成员与浮点数分值之间有序映射元素的顺序由分值⼤⼩决定

命令：zadd、zrange、zrangebyscore、zrem

### 数据结构

跳表（skiplist）：

Redis zset 是⼀个复合结构，⼀⽅⾯它需要⼀个 hash 结构来存储 value和 score 的对应关系，另⼀⽅⾯需要提供按照 score 排序的功能，还需要能够指定 score 的范围来获取 value 列表的功能。所以需要跳表来帮忙实现

Redis 的跳跃列表共有64 层，可容纳 2 ^ 64 个元素

kv 之间使⽤指针串起来形成了双向链表结构，它们是有序排列的，从⼩到⼤。不同的 kv 层⾼可能不 样，层数越⾼ 的 kv 越少。同⼀层的 kv 会使⽤指针串起来。每⼀个层元素的遍历都是从 kv header 出发

Hash：

字典，又称为符号表( symbol table )、关联数组( associative array)或映射(map), 是⼀种⽤于保存键值对( key-value pair)的抽象数据结构，Redis的字典是使⽤哈希作为底层实现的

哈希对象的编码可以是ziplist或者hashtable。当哈希对象可以同时满⾜以下两个条件时，哈希对象使⽤ziplist编码:

​	1.哈希对象保存的所有键值对的键和值的字符串长度都⼩于64字节

​	2.哈希对象保存的键值对数量⼩于512个

不能满⾜这两个条件的哈希对象需要使⽤hashtable编码



## Redis分片

将数据拆分到多个redis实例的过程 

功能：

​	1.允许使⽤很多电脑的内存总和来⽀持更⼤的数据库

​	2.允许伸缩计算能⼒到多核或者多服务器，伸缩⽹络带宽到多服务器或者多⽹络适配器 

方式（哈希分片）：

​	1.使⽤哈希函数对键名转换为⼀个数字

​	2. 对这个数字进⾏取模，取模的基数等于redis实例的对象 



## Redis旁路缓存

缓存的场景：

​	1.不需要实时更新又极其消耗数据库的数据

​	2.需要实时更新，但更新的频率不⾼

简单来说，就是需要在应⽤程序中新增缓存逻辑处理的代码。Redis就是旁路缓存，因为需要应⽤程序调⽤它。电脑内存⾥的磁盘⽂件就不是旁路缓存，因为是⾃动调⽤的，对应⽤程序透明。

Redis做缓存的两种模式：

​	1.只读缓存：

​		加强读请求性能。查询数据时，缓存缺失需要从DB加载。更新数据时到DB更新，Redis上的⽼数据直接删除。

​	2.读写缓存：

​		读操作和只读缓存⼀样。写操作分为同步直写和异步写回两种模式，根据实际的业务场景需求来进⾏选择：

​			1.同步直写模式

​				Redis和DB同时删改写回。侧重于保证数据可靠性。 

​			2.异步写回模式

​				只写Redis，数据要被淘汰时再写回DB（淘汰指的是系统空间不足，需要腾出空间时再写入）。侧重于提供低延迟访问。



## Redis替换策略

在缓存满时，Redis公有8种淘汰策略

1.不进行数据淘汰（1种）

​	no-enviction：禁⽌驱逐数据，这种情况下会直接导致写入错误

2.进行数据淘汰

​	（1）设置过期时间对数据进行淘汰（4种）

​		volatile-ttl、volatile-random、volatile-lru、volatile-lfu

​		volatile-ttl：从已设置过期时间的数据集中，选择将要过期的数据进⾏淘汰

​		volatile-random：从已设置过期时间的数据集中，随机选择数据进⾏淘汰

​		volatile-lru：从已设置过期时间的数据集中，挑选最近最少使⽤的数据淘汰

​		volatile-lfu：从已设置过期时间的数据集中，挑选最不经常使⽤的数据淘汰（相较于lru的频率算法策略不一样）

​	（2）所有数据范围内进行淘汰（3种）

​		allkeys-random、allkeys-lru、allkeys-lfu

​		相比于上面唯一的区别就是在整个数据集中

传统的LRU算法，需要维护⼀个⼤链表，随着数据访问，更新数据在链表中的位置。 

Redis 中对 LRU 算法进⾏了简化，简单来说就是增加了⼀个随机选取候选集合的操作，具体如下：

​	1.记录每个数据的访问时间。Redis 默认会记录每个数据的最近⼀次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）

​	2.随机选取⼀个集合。Redis 在决定淘汰的数据时，第⼀次会随机选出 N 个数据，把它们作为⼀个候选集合

​	3.淘汰集合中lru字段最⼩的数据。接下来，Redis 会⽐较这 N 个数据的 lru 字段，把 lru 字段值最⼩的数据从缓存中淘汰出去

​	4.需要再次淘汰数据时。选取集合外，lru 字段值⼩于候选集合中最⼩的 lru 值的数据，进⼊候选集合，直到选满集合。重复步骤3。

这样，Redis就不⽤维护⼀个⼤链表了，也不⽤频繁的进⾏链表数据交换操作。 

Redis缓存⼤⼩选取建议：结合实际应⽤的数据总量、热数据的体量，以及成本预算，设置在总数据量的 15% 到 30% 这个区间。



## Redis备份

持久化机制：RDB，AOF

RDB：在指定的时间间隔内将内存中的数据集快照写⼊磁盘 

AOF：以⽇志的形式记录服务器所处理的每⼀个写操作，redis服务器启动之初，会读取该⽇志来重新构建数据库，以保证启动后的数据库是完整的。

### AOF

AOF ⽇志是写后⽇志，"写后" 的意思是 Redis 是先执⾏命令，把数据写⼊内存，然后才记录⽇志，Redis是内存和 ⽇志(写后⽇志)，mysql是磁盘数据和⽇志(写前⽇志)

Redis 使⽤写后⽇志这⼀⽅式的好处：

​	1.可以避免出现记录错误命令的情况。

​	2. 它是在命令执⾏后才记录⽇志，所以不会阻塞当前的写操作。

AOF三种写回策略：

​	Always，同步写回；Everysec，每秒写回；No，操作系统控制的写回

![image-20231221170505340](C:\Users\Vica\AppData\Roaming\Typora\typora-user-images\image-20231221170505340.png)

​	想要获得⾼性能，就选择 No 策略；如果想要得到⾼可靠性保证，就选择 Always 策略；如果允许数据有⼀点丢 失，又希望性能别受太⼤影响的话，那么就选择 Everysec 策略。

AOF重写：

​	AOF通过AOF重写的⽅式进⾏减⼩AOF⽂件⼤⼩，优化存储结构

​	AOF重写过程：⼀处拷贝，两处⽇志

![image-20231221175727701](C:\Users\Vica\AppData\Roaming\Typora\typora-user-images\image-20231221175727701.png)

​	AOF有四个触发时机：

​		1.bgrewriteaof 命令被执⾏

​		2.主从复制完成 RDB ⽂件解析和加载（⽆论是否成功）

​		3.AOF 重写被设置为待调度执⾏

​		4.AOF 被启⽤，同时 AOF ⽂件的⼤⼩⽐例超出阈值，以及 AOF ⽂件的⼤⼩绝对值超出阈值。

在这四个时机下，都不能有正在执⾏的 RDB ⼦进程和 AOF 重写⼦进程，否则的话， AOF 重写⽆法执⾏

AOF 重写和 RDB 创建的过程类似，它也是创建了⼀个⼦进程来完成重写⼯作。这是因为 AOF 重写操作，实际上需要遍历 Redis server 上的所有数据库，把每个键值对以插⼊操作的形式写⼊⽇志⽂件，⽽⽇志⽂件又要进⾏写盘操作。所以，Redis 源码使⽤⼦进程来实现 AOF 重写，这就避免了阻塞主线程，也减 少了对 Redis 整体性能的影响。



## Redis扩容

集群：动态增加 Redis 节点

主从同步，读写分离：对 Redis 的访问分为读和写

hash算法：分成多个实例进⾏存储，增加 Redis 服务器数量。在客户端对存储的 key 进⾏ hash 运算，存⼊不同的服务器中， 读取时，也进⾏相同的 hash 运算，找到对应的 redis 服务器



## Redis主从复制

Redis 提供了主从库模式，以保证数据副本的⼀致，主从库之间采⽤的是读写分离的⽅式

读操作：主库、从库都可以接收

写操作：⾸先到主库执⾏，然后，主库将写操作同步给从库

全量复制/增量复制：

​	全量复制为主从库间进行的第一次复制操作

![image-20231221180834651](C:\Users\Vica\AppData\Roaming\Typora\typora-user-images\image-20231221180834651.png)

​	第⼀阶段是从库和主库建⽴起连接，并告诉主库即将进⾏同步，主库确认回复后，主从库间就可以开始同步了

​	第⼆阶段主库收到 psync 命令后，会⽤ FULLRESYNC 响应命令带上两个参数：主库 runID 和主库⽬前的复制进度 offset，返回给从库

​	第三个阶段，主库会把第⼆阶段执⾏过程中新收到的写命令，再发送给从库



​	增量复制发生在从库发生宕机，重新连接后

![image-20231221181207984](C:\Users\Vica\AppData\Roaming\Typora\typora-user-images\image-20231221181207984.png)

​	若从库发⽣宕机，主库会把断连期间收到的写操作命令，写到repl_backlog_buffer中，当从库重连后，从库⾸先会 给主库发送 psync 命令，并把⾃⼰当前的 slave_repl_offset 发给主库，主库会判断⾃⼰的 master_repl_offset 和 slave_repl_offset 之间的差距。若此时从库相差 > repl_backlog_buffer说明可能⼀个从库如果和主库断连时间过 长，不能够进⾏增量复制(因为前⾯的值被覆盖的了，会导致数据不⼀致)，所以直接进⾏全量复制

​	⽽从库相差 < repl_backlog_buffer，则将master_repl_offset 和 slave_repl_offset 之间的差距发送给从库进⾏执⾏

Redis 的主从库同步的基本原理，总结来说，有三种模式：全量复制、基于长连接的命令传播，以及增量复制

​	第⼀次同步(本来就是全量复制)或者增量复制时master_repl_offset 和 slave_repl_offset差值>repl_backlog_buffer 时(repl_backlog_size这个配置参数) 就⽆法增量复制，就被迫导致全量复制

​	主从库正常运⾏后的常规同步阶段，在这个阶段中，主从库之间通过命令传播实现同步。

​	从库断开重连后进⾏增量复制

主从全量同步使⽤RDB⽽不使⽤AOF的原因：

​	1.RDB⽂件内容是经过压缩的⼆进制数据（不同数据类型数据做了针对性优化），⽂件很⼩。⽽AOF⽂件记录的 是每⼀次写操作的命令，写操作越多⽂件会变得很⼤，其中还包括很多对同⼀个key的多次冗余操作

​	2.打开AOF就要选择⽂件刷盘的策略，选择不当会严重影响Redis性能。⽽RDB只有在需要定时备份和主从全量同步数据时才会触发⽣成⼀次快照



## Redis其他

### Redis容灾

在采用主从复制的方式时，从库同时开启快照和AOF进行持久化，保证数据的安全性

当主库挂掉时，将从库升级为主库；在原主库数据恢复后，将其降级为从库并启动

如果主库和从库都挂掉，可以调用命令通过AOF和快照进行回复

### Redis哨兵

监控主从是否正常，在出现问题时会自动通知

故障迁移：包括自动主从切换，以及统一的配置管理（获取主从地址）

### Redis消息队列

Redis做消息队列，使用list实现， lpush, rpush操作实现⼊队，lpop, rpop 来实现出队。

在客户端，维护一个死循环来从队列中读取消息并处理，如果队列中有消息，就读取，没有消息，就会陷⼊死循 环，知道下⼀次有消息进⼊，这种死循环会造成⼤量的资源浪费，可以使⽤ lbpop,也就是阻塞式弹出

延迟消息队列⽤ Zset实现，其中 Zset 的score 就是时间。 将 value 存到 redis 中，然后通过 轮询的⽅式，去不断的读取消息出来。然后 处理。

### 布隆过滤器

专门⽤来处理去重问题，它相当于⼀个不那么精确的 set 集合，可以利⽤它的 contains ⽅法去判断某⼀个对象是否存在，但是判断结果不是很精确，如果 contains 判断某个值不存在，那就⼀定不存在，如果判断为存在，那么不⼀定存在。

每⼀个布隆过滤器在Redis 中都对应了⼀个⼤型的位数组以及⼏个不同的 hash 函数

add操作根据⼏个不同的 hash 函数给元素进⾏ hash 运算⼀个整数索引值，拿到这个索引值之后，对位数组的长度进⾏取模运算，得到⼀个位置，每⼀个 hash 函数都会得到⼀个位置，将位数组中对应的位置设置位 1 ，这样就完成了添加操作

当判读元素是否存在时，同样的。先对元素进⾏ hash 运算，将运算的结果和位数组取模，然后去对应的位置查看 是否有相应的数据，如果有，表⽰元素可能存在（因为这个有数据的地⽅也可能是其他元素存 进来的），如果没有 表⽰元素⼀定不存在。

Bloom Filter 中，误判的概率和位数组的⼤⼩有很⼤关系，位数组越⼤，误判概率越⼩，当然占⽤的存储空间越⼤；位数组越⼩，误判概率越⼤，当然占⽤的存储空间就⼩。















